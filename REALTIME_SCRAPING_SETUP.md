# ğŸš€ Real-Time Scraping Setup Guide

This guide will help you set up and run real-time scraping with your new credentials management system.

## **ğŸ“‹ Prerequisites**

1. âœ… **Database Table Created** - Run the SQL script in Supabase
2. âœ… **Environment Variables Set** - Supabase and Browserless.io configured
3. âœ… **Credentials Added** - At least one account in the credentials table

## **ğŸ—„ï¸ Step 1: Create Database Table**

Run this SQL in your **Supabase SQL Editor**:

```sql
CREATE TABLE IF NOT EXISTS credentials (
    account_name TEXT PRIMARY KEY,
    email TEXT NOT NULL,
    password TEXT NOT NULL,
    pin TEXT NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_credentials_email ON credentials(email);
ALTER TABLE credentials ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can manage their own credentials" ON credentials
    FOR ALL USING (auth.uid() IS NOT NULL);

CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_credentials_updated_at 
    BEFORE UPDATE ON credentials 
    FOR EACH ROW 
    EXECUTE FUNCTION update_updated_at_column();
```

## **ğŸ” Step 2: Add Credentials**

1. **Start the dev server**: `npm run dev`
2. **Navigate to**: `http://localhost:3000/credentials`
3. **Add your credentials**:
   - **Account Name**: `groww_main` (or any name you prefer)
   - **Email**: Your Groww email
   - **Password**: Your Groww password
   - **PIN**: Your 5-digit Groww PIN

## **ğŸš€ Step 3: Start Real-Time Scraping**

### **Quick Start (30-minute intervals)**
```bash
npm run scrape:realtime groww_main
```

### **Custom Intervals**
```bash
# Scrape every 15 minutes
npm run scrape:realtime groww_main 15

# Scrape every 30 minutes with 5 retries
npm run scrape:realtime groww_main 30 5

# Scrape every hour
npm run scrape:realtime groww_main 60
```

### **Test Run (1-minute interval, 1 retry)**
```bash
npm run scrape:test
```

### **Single Scrape**
```bash
npm run scrape:once
```

## **ğŸ” Step 4: Handle OTP (When Required)**

When the scraper asks for OTP:

### **Method 1: Using NPM Script**
```bash
npm run otp:provide 123456
```

### **Method 2: Using Node.js**
```bash
node -e "
const { provideOTP } = require('./src/lib/otpHandler');
provideOTP('groww-session', '123456');
"
```

### **Check OTP Status**
```bash
npm run otp:status
```

## **ğŸ“Š Step 5: Monitor Scraping**

The real-time scraper will:

- âœ… **Log progress** with timestamps
- âœ… **Show holdings summary** after each scrape
- âœ… **Calculate total portfolio value**
- âœ… **Handle errors gracefully** with retries
- âœ… **Wait for OTP input** when required

### **Sample Output**
```
[realtime-scraper] Starting real-time scraping for account: groww_main
[realtime-scraper] Interval: 30 minutes
[realtime-scraper] Max retries: 3
[realtime-scraper] Starting scrape at 2024-01-15T10:30:00.000Z
[realtime-scraper] Progress: 15% - connecting to browserless (free plan)
[realtime-scraper] Progress: 40% - login successful, navigating to holdings
[realtime-scraper] Progress: 90% - extraction complete - 25 holdings found
[realtime-scraper] âœ… Scrape completed successfully!
[realtime-scraper] ğŸ“Š Holdings found: 25
[realtime-scraper] â±ï¸ Duration: 45.23 seconds
[realtime-scraper] ğŸ’° Total value: â‚¹8,71,665
[realtime-scraper] ğŸ“‹ Holdings Summary:
[realtime-scraper]   1. TCS: 100 shares @ â‚¹3,850 = â‚¹3,85,000
[realtime-scraper]   2. Infosys: 200 shares @ â‚¹1,500 = â‚¹3,00,000
[realtime-scraper] Waiting 30 minutes until next scrape...
```

## **ğŸ› ï¸ Troubleshooting**

### **Common Issues**

#### **1. "No credentials found"**
- âœ… Check if credentials are added in `/credentials` page
- âœ… Verify account name matches exactly
- âœ… Check database connection

#### **2. "OTP timeout"**
- âœ… Provide OTP within 60 seconds
- âœ… Use `npm run otp:provide <otp>` command
- âœ… Check if OTP is correct

#### **3. "Browserless connection failed"**
- âœ… Verify `BROWSERLESS_TOKEN` is set
- âœ… Check Browserless.io account status
- âœ… Ensure free plan limits aren't exceeded

#### **4. "Database connection error"**
- âœ… Verify Supabase environment variables
- âœ… Check if credentials table exists
- âœ… Ensure RLS policies are correct

### **Debug Commands**

```bash
# Check environment variables
echo $NEXT_PUBLIC_SUPABASE_URL
echo $SUPABASE_SERVICE_ROLE_KEY
echo $BROWSERLESS_TOKEN

# Test database connection
npm run scrape:once

# Check OTP status
npm run otp:status

# View logs
tail -f logs/scraping.log
```

## **âš™ï¸ Configuration Options**

### **Scraping Intervals**
- **Frequent**: 15 minutes (for active trading)
- **Standard**: 30 minutes (recommended)
- **Conservative**: 60 minutes (for long-term tracking)

### **Retry Settings**
- **Conservative**: 3 retries (default)
- **Aggressive**: 5 retries (for unstable connections)
- **Minimal**: 1 retry (for testing)

### **OTP Handling**
- **Automatic**: Set `enableOTPHandling: true` (default)
- **Manual**: Set `enableOTPHandling: false` (for headless operation)

## **ğŸ”’ Security Notes**

- âœ… **Credentials encrypted** in database
- âœ… **No credential logging** in console
- âœ… **Secure API routes** with validation
- âœ… **Row-level security** enabled
- âœ… **Environment variables** for sensitive data

## **ğŸ“ˆ Performance Tips**

1. **Optimal Intervals**: 30 minutes for most use cases
2. **Retry Strategy**: 3 retries with exponential backoff
3. **Resource Management**: Automatic cleanup after each scrape
4. **Error Handling**: Graceful degradation on failures

## **ğŸš€ Production Deployment**

For production deployment:

1. **Set environment variables** in your hosting platform
2. **Use PM2 or similar** for process management
3. **Set up monitoring** for scraping health
4. **Configure logging** for debugging
5. **Set up alerts** for failures

### **PM2 Example**
```bash
npm install -g pm2
pm2 start "npm run scrape:realtime groww_main 30" --name "groww-scraper"
pm2 save
pm2 startup
```

## **ğŸ“ Support**

If you encounter issues:

1. **Check logs** for detailed error messages
2. **Verify setup** using the troubleshooting guide
3. **Test components** individually
4. **Review documentation** in project files

**Happy Scraping! ğŸ‰**
